---
title: "PS3"
format:
  html:
    code-overflow: wrap
    embed-resources: true
    theme: simplex
---

## Problem 1

a. Print out the dimensions of the merged data.frame.
```{r}
# rm(list=ls())

# install.packages("haven")
library(haven)
aux <- read_xpt("data/AUX_I.xpt")
demo <- read_xpt("data/DEMO_I.xpt")

# both aux and demo share the "SEQN" column
data <- merge(aux, demo, by="SEQN")
dim(data)
```

b. Clean up variables.

Firstly, I rename the columns of the specified variables for convenience and easier understanding. I also rename the columns for Tympanometric width in each ear in this section as those columns will be needed in part c. 
```{r}
names(data)[names(data) == "RIAGENDR"] <- "Gender"
names(data)[names(data) == "DMDCITZN"] <- "Citizenship"
names(data)[names(data) == "DMDHHSZA"] <- "NumChildren"
names(data)[names(data) == "INDHHIN2"] <- "Income"

# this is for part c
names(data)[names(data) == "AUXTWIDR"] <- "WidthR"
names(data)[names(data) == "AUXTWIDL"] <- "WidthL"

vars <- c("WidthR", "WidthL", "Gender", "Citizenship", "NumChildren", "Income")
summary(data[vars])
```
In this next section, I do some data cleaning to remove values that may not be valid. 

From the summary table above, we can see that there are NA values, so I remove observations that contain NA values, to be more specific, I omit observations with at least one NA value in the specified columns.
```{r}
# omit NAs
data_clean <- na.omit(data[vars])
```
Next, I looked at and cleaned up each of the variables. For Gender and NumChildren, there are no values that seem out of place, so nothing is done to it.
```{r}
unique(data_clean$Gender)
unique(data_clean$NumChildren)
```
For citizenship status, I removed the values 7 and 9. According to the documentation, these values represented "refused" and "don't know" respectively. 
```{r}
# Citizenship status: remove "refused (7)" and "don't know (9)"
data_clean <- data_clean[!data_clean$Citizenship %in% c(7, 9), ]
```
For annual household income, I removed the values 77 and 99, as these values represented "refused" and "don't know" respectively. Upon reading the documentation, we find out that the values 12 and 13 are out of order, as these values represent "$20,000 and Over" and "$Under $20,000" respectively. These are out of order as the values 1-4 also represent annual incomes under $20,000, and values 5 and above represent annual incomes over $20,000. Since the groupings of "Under $20,000" and "$20,000 and Over" don't give much insight into where we should re-categorize it into the other values, and that the number of respondents who gave either of the two answers are a minority compared to the total number of respondents, I chose to remove the rows with values 12 and 13. I then reordered the rows, pushing the values of 14 and 15 up to 11 and 12 respectively to preserve continuity.
``` {r}
# remove "refused (77)" and "don't know (99)"
data_clean <- data_clean[!data_clean$Income %in% c(77, 99), ]
# remove 12 and 13 as they are out of order
data_clean <- data_clean[!data_clean$Income %in% c(12, 13), ]
data_clean <- data_clean[!data_clean$Income %in% c(12, 13), ]
# reorder 14,15 to 11,12 
data_clean$Income[data_clean$Income == 14] <- 11
data_clean$Income[data_clean$Income == 15] <- 12
```
Lastly, I convert categorical variables to factors with informative levels. I chose to convert gender and citizenship status, renaming the levels for each to "M" and "F", and "Citizen" and "Not Citizen" to be more informative than 1 and 2. I did not convert NumChildren and Income, as I decided that these variables could be said to be ordinal.
``` {r}
# now, convert to factor with informative levels
data_clean$Gender <- factor(
  data_clean$Gender,
  levels = c(1, 2),
  labels = c("M", "F"),
  ordered = FALSE
)

data_clean$Citizenship <- factor(
  data_clean$Citizenship,
  levels = c(1, 2),
  labels = c("Citizen", "Not Citizen"),
  ordered = FALSE
)
```
After these data cleaning steps, the summary table is as shown below.
``` {r}
summary(data_clean)
```

c. Fit four Poisson regression models predicting a respondent’s Tympanometric width in each ear. Produce a table presenting the estimated incidence risk ratios for the coefficients in each model, along with the sample size for the model, the pseudo-R2, and AIC 
```{r}
# create the models
model_1R <- glm(WidthR ~ Gender,
                family = poisson, data=data_clean)
model_2R <- glm(WidthR ~ Gender + Citizenship + NumChildren + Income,
                family = poisson, data=data_clean)
model_1L <- glm(WidthL ~ Gender,
                family = poisson, data=data_clean)
model_2L <- glm(WidthL ~ Gender + Citizenship + NumChildren + Income,
                family = poisson, data=data_clean)

# calculate the pseudo r2
pseudor2_1R <- 1 - (model_1R$deviance / model_1R$null.deviance)
pseudor2_2R <- 1 - (model_2R$deviance / model_2R$null.deviance)
pseudor2_1L <- 1 - (model_1L$deviance / model_1L$null.deviance)
pseudor2_2L <-  1 - (model_2L$deviance / model_2L$null.deviance)

# install.packages("stargazer")
library(stargazer)

stargazer(model_1R, model_2R, model_1L, model_2L,
                 type = "text",
                 apply.coef = exp,
                 add.lines = list(c("Pseudo R2",
                                    pseudor2_1R,
                                    pseudor2_2R,
                                    pseudor2_1L,
                                    pseudor2_2L)),
                 digits = 3)
```
The warnings are likely because 1R and 1L have a different number of predictors compared to 2R and 2L, and can thus be ignored.

d.  From model 2L, provide evidence whether there is a difference between males and females in terms of their incidence risk ratio. Test whether the predicted value of Tympanometric width measure of the left ear differs between men and women. Include the results of the each test and their interpretation.
```{r}
# test difference in terms of IRR
summary(model_2L)$coefficients["GenderF", ]
```
The p-value is very small, \< 0.05, so there is significant difference between males and females according to their incidence risk ratio.

```{r}
# test predicted
data_M <- data_clean[data_clean$Gender == "M", ]
data_F <- data_clean[data_clean$Gender == "F", ]

pred_M   <- predict(model_2L, newdata = data_M, type = "response")
pred_F <- predict(model_2L, newdata = data_F, type = "response")
t.test(pred_M, pred_F)
```
The p-value is very small, \< 0.05, so there is significant difference between males and females according to their predicted value.

## Problem 2
```{r}
# install.packages("DBI")
library(DBI)

sakila <- dbConnect(RSQLite::SQLite(), "data/sakila_master.db")

# function for convenient query requests
gg <- function(query) {
  dbGetQuery(sakila, query)
}
```

a.  For each store, how many customers does that store have, and what percentage of customers of that store are active in the system?
```{r}
library(microbenchmark)

a_r <- function() {
  df1 <- gg("SELECT * FROM customer")
  num_customers <- tapply(df1$customer_id, df1$store_id, length)
  percent_active <- 100 * tapply(as.numeric(df1$active), df1$store_id, sum) / num_customers
  
  out <- data.frame(
    store_id = names(num_customers),
    num_customers = num_customers,
    percent_active = percent_active
  )
  
  return(out)
}

a_sql <- function() {
  gg("
   SELECT store_id,
          COUNT(customer_id) as num_customers,
          100*SUM(active)/COUNT(customer_id) as percent_active
   FROM customer
   GROUP BY store_id
  ")
}

list(a_r(), a_sql())

microbenchmark(a_r, a_sql, times=1000)
```
Sometimes, the SQL method is faster than the R method, but sometimes it's vice versa. To investigate this, I found the proportion of times SQL is faster than R, and performed a t-test to test the difference in mean runtimes. These tests can be found at the end of Q3. I concluded that there is no statistically significant difference between SQL and R runtimes for the methods used in Q3. Also, take note that microbenchmark failed to get a runtime for a fairly significant portion of the number of times it was run. This will be a problem for the subsequent subquestions as well, which may mean that the results of the microbenchmark are not entirely accurate. Technically though, SQL should be faster than R, but the difference would be more significant at larger amounts of data.

b.  Generate a table identifying the names and country of each staff member.
```{r}

b_r <- function() {
  # to reduce the number of tables needed to be retrieved, I use "join" in the query
  df2 <- gg("SELECT * FROM staff AS s 
              LEFT JOIN address AS a ON s.address_id = a.address_id
              LEFT JOIN city AS ci ON a.city_id = ci.city_id
              LEFT JOIN country AS co ON ci.country_id = co.country_id
            ")
  
  return(df2[, c("first_name", "last_name", "country")])
}

b_sql <- function() {
  
  gg("SELECT first_name, last_name, country FROM staff AS s 
        LEFT JOIN address AS a ON s.address_id = a.address_id
        LEFT JOIN city AS ci ON a.city_id = ci.city_id
        LEFT JOIN country AS co ON ci.country_id = co.country_id
     ")
}

list(b_r(), b_sql())

microbenchmark(b_r, b_sql, times=1000)
```
Similar to part (a), sometimes SQL is faster than R, and vice versa.

c.  Identify the name(s) of the film(s) which was/were rented for the highest dollar value. (Assume all costs are in USD regardless of country.) (Hint: You can merge a table more than once.)
```{r}

c_r <- function() {
  df3 <- gg("SELECT * FROM payment AS p
              LEFT JOIN rental AS r ON p.rental_id = r.rental_id
              LEFT JOIN inventory AS i ON r.inventory_id = i.inventory_id
              LEFT JOIN film AS f ON i.film_id = f.film_id
            ")
  
  highest_rent <- df3[df3$amount == max(df3$amount), ]
  out <- highest_rent[, c("title", "amount")]
  
  return(out)
}

c_sql <- function() {
  gg("SELECT title, amount FROM film AS f
        RIGHT JOIN inventory AS i ON f.film_id = i.film_id
        RIGHT JOIN rental AS r ON i.inventory_id = r.inventory_id
        RIGHT JOIN 
        (SELECT rental_id, amount FROM payment
          WHERE amount == (SELECT MAX(amount) FROM payment)
        ) AS pp ON r.rental_id = pp.rental_id
     ")
}

list(c_r(), c_sql())
microbenchmark(c_r, c_sql, times=1000)
```
Similarly, sometimes SQL is faster than R, and vice versa.

While running the code, I saw that, contrary to the technical expectation that SQL would be faster than R, the R method would be faster than the SQL method sometimes. Just for fun, I decided to check how often the mean runtime SQL is actually faster than R (using something like Monte Carlo)
```{r}
check_sql_faster <- function(r, sql, reps=100, times=100) {
  out <- replicate(reps, {
    mb <- suppressWarnings(microbenchmark(r, sql, times=times))
    df <- as.data.frame(mb)
    mean_r <- mean(df$time[df$expr == "r"])
    mean_sql <- mean(df$time[df$expr == "sql"])
    mean_sql < mean_r
  })
  
  return(mean(out))
}

list(check_sql_faster(a_r, a_sql),
     check_sql_faster(b_r, b_sql),
     check_sql_faster(c_r, c_sql))
```
Interestingly, from the above, we actually see that SQL is faster than R only about half of the time. It could be that these are short and easy queries for both R and SQL, so their runtimes are not very different.
```{r}
test_sql_faster <- function(r, sql, times=100) {
  mb <- suppressWarnings(microbenchmark(r, sql, times=times))
  df <- as.data.frame(mb)
  data_r <- df$time[df$expr == "r"]
  data_sql <- df$time[df$expr == "sql"]
  
  return(t.test(data_r, data_sql))
}

list(test_sql_faster(a_r, a_sql),
    test_sql_faster(b_r, b_sql),
    test_sql_faster(c_r, c_sql))
```
Indeed, using a t-test, we see that there is no statistically significant difference in the mean runtimes between R and SQL methods.

## Problem 3

```{r}
# load the data
aus_data <- read.csv("data/au-500.csv")
```

a.  What percentage of the websites are .com’s (as opposed to .net, .com.au, etc)?
```{r}
grep("\\.com$", aus_data$web)
```
There seems to be no ".com"s in the data, so the percentage is 0.

b.  What is the most common domain name amongst the email addresses? (In the email “statistics\@umich.edu”, “umich.edu” is the domain name.)
```{r}
domain_names <- sub(".*@", "", aus_data$email)
head(sort(table(domain_names), decreasing=TRUE))
```
hotmail.com is the most common domain name.

c.  What proportion of company names contain a non-alphabetic character, excluding commas and whitespace. (E.g. “Jane Doe, LLC” would not contain an eligible non-alphabetic character; “Plumber 247” would.) What about if you also exclude ampersands (“&”)?
```{r}
length(grep("[^a-zA-Z, ]", aus_data$company_name)) / length(aus_data$company_name)
length(grep("[^a-zA-Z,& ]", aus_data$company_name)) / length(aus_data$company_name)
```
Only 9% of company names contain a non-alphabetic character (excluding commas and whitespace). If we also exclude ampersands, then only 0.8% of company names fit the criteria.

d.  In Australia, phone have 10 digits - but unlike in the US where we write all numbers as “123-456-7890”, they write land lines and cell phones differently. There are two different phones listed for each record. Make all phone numbers written like cell phones. Show it works by printing the first 10 phone numbers of each column.
```{r}
# sub all - for blank spaces

aus_data$phone1_cell <- gsub("-", "", aus_data$phone1)
aus_data$phone2_cell <- gsub("-", "", aus_data$phone2)

# insert - after 4, then 3 characters
aus_data$phone1_cell <- gsub("^([0-9]{4})([0-9]{3})(.*)", "\\1-\\2-\\3", aus_data$phone1_cell)
aus_data$phone2_cell <- gsub("^([0-9]{4})([0-9]{3})(.*)", "\\1-\\2-\\3", aus_data$phone2_cell)

head(aus_data$phone1, 10)
head(aus_data$phone1_cell, 10)
head(aus_data$phone2, 10)
head(aus_data$phone2_cell, 10)
```

e.  Produce a histogram of the log of the apartment numbers for all addresses. (You may assume any number at the end of the an address is an apartment number.)
```{r}
# get all addresses that end in an apartment number
apt_addresses <- aus_data$address[grepl("^.*([0-9]+)$", aus_data$address)]
head(apt_addresses)
```
From seeing some observations that end in a number, we can see that the apartment numbers are usually separated by either a whitespace " ", or a "\#" symbol, so I extract the apartment numbers by using \[ #\] as a separator.

```{r}
# extract only the apartment numbers
apt_nums <- sub("^.*[# ]([0-9]+)$", "\\1", apt_addresses)

# take the log and plot
log_apt_nums <- log(as.numeric(apt_nums))
hist(log_apt_nums)
```

f.  Benford’s law is an observation about the distribution of the leading digit of real numerical data. Examine whether the apartment numbers appear to follow Benford’s law. Do you think the apartment numbers would pass as real data?
```{r}
# get the first digits of the data
first_digit_apt_nums <- as.numeric(substr(apt_nums,1,1))
hist(first_digit_apt_nums)
hist(log(first_digit_apt_nums))
```

From the frequency histogram, the most frequent leading digit is one, the rest of the digits do not seem to decrease in frequency as is suggested by Benford's law. Furthermore, the histogram of the log of the leading digits does not appear to be particularly uniform, which is another indication that it does not follow Benford's law. By Benford's law, it can be said that this data may not pass as real data, however, it may be noted that not all real datasets follow Benford's law either. In this case, it is indeed true that the data is artificially generated, but Benford's law is not a strict rule to determine if data is real.

## Attribution of Sources

-   How to read XPT files:[https://www.rdocumentation.org/packages/haven/versions/2.0.0/topics/read_xpt](https://www.rdocumentation.org/packages/haven/versions/2.0.0/topics/read_xpt)

-   Merging dataframes: [https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/merge](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/merge)

-   Poisson models: [https://stats.oarc.ucla.edu/r/dae/poisson-regression/](https://stats.oarc.ucla.edu/r/dae/poisson-regression/)

-   Understanding stargazer: [https://www.rdocumentation.org/packages/stargazer/versions/5.2.3/topics/stargazer](https://www.rdocumentation.org/packages/stargazer/versions/5.2.3/topics/stargazer)

-   Q3(d): [https://stackoverflow.com/questions/38704603/how-to-add-a-character-to-a-string-in-r](https://stackoverflow.com/questions/38704603/how-to-add-a-character-to-a-string-in-r)

-   Pseudo R2:[https://stats.stackexchange.com/questions/11676/pseudo-r-squared-formula-for-glms](https://stats.stackexchange.com/questions/11676/pseudo-r-squared-formula-for-glms)

-   Manually Specifying Pseudo R2 in Stargazer:[https://stackoverflow.com/questions/44318860/r-stargazer-manually-specify-r2-and-write-to-tex](https://stackoverflow.com/questions/44318860/r-stargazer-manually-specify-r2-and-write-to-tex)
